{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d351eb",
   "metadata": {},
   "source": [
    "### Goles\n",
    "\n",
    "- Automate the process of optimizing the parametes w and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f47b27",
   "metadata": {},
   "source": [
    "### Import the required modules\n",
    "\n",
    "- Numpy - For mathamatical computation\n",
    "- Matplotlib - For Ploting the data in graph format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b15af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b4863",
   "metadata": {},
   "source": [
    "### Write the function for univariate Gradient Descent\n",
    "- Simultanneously update the parameter w and b\n",
    "$$\\begin{align} \\text{repeat until convergence }\\lbrace \\newline w = w - \\alpha \\frac{\\partial J(w, b)}{\\partial w} \\newline\n",
    "b = b - \\alpha \\frac {\\partial J(w, b)}{\\partial b} \\newline \\rbrace \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e98cf",
   "metadata": {},
   "source": [
    "### Where \n",
    "$$\n",
    "\\begin{align} \\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "it is defined as the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256ecf7",
   "metadata": {},
   "source": [
    "### First create a function called compute gradient which will compute the gradient upon call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3accfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b):\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw += (f_wb - y[i]) * x[i]\n",
    "        dj_db += f_wb - y[i]\n",
    "        \n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4a823",
   "metadata": {},
   "source": [
    "### Create a function called compute cost which will calculate the cost of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34007517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "    cost = 0\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost += (f_wb - y[i]) ** 2\n",
    "    \n",
    "    cost /= 2\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f9a08c",
   "metadata": {},
   "source": [
    "### write a function for gradient descent using the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66afb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha = 0.001, iteration=1000):\n",
    "    j_history = []\n",
    "    p_history = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = copy.deepcopy(b_in)\n",
    "    \n",
    "    for i in range(iteration):\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "        j = cost_function(x, y, w, b)\n",
    "        \n",
    "        tmp_w = w - alpha * dj_dw\n",
    "        tmp_b = b - alpha * dj_db\n",
    "        \n",
    "        w = tmp_w\n",
    "        b = tmp_b\n",
    "        \n",
    "        if i < 10000:\n",
    "            j_history.append(j)\n",
    "            p_history.append([w, b])\n",
    "    \n",
    "        if i % math.ceil(iteration / 10) == 0:\n",
    "            print(f\"iteration: {i:.2f} | cost: {j:.2f} | w: {w:.2f}, b: {b:.2f} | dj_dw: {dj_dw:.2f}, dj_db: {dj_db:.2f}\")\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75d80ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0.00 | cost: 170000.00 | w: 6.50, b: 4.00 | dj_dw: -650.00, dj_db: -400.00\n",
      "iteration: 1000.00 | cost: 6.83 | w: 194.91, b: 108.23 | dj_dw: -0.37, dj_db: 0.60\n",
      "iteration: 2000.00 | cost: 1.59 | w: 197.55, b: 103.97 | dj_dw: -0.18, dj_db: 0.29\n",
      "iteration: 3000.00 | cost: 0.37 | w: 198.82, b: 101.91 | dj_dw: -0.09, dj_db: 0.14\n",
      "iteration: 4000.00 | cost: 0.09 | w: 199.43, b: 100.92 | dj_dw: -0.04, dj_db: 0.07\n",
      "iteration: 5000.00 | cost: 0.02 | w: 199.73, b: 100.44 | dj_dw: -0.02, dj_db: 0.03\n",
      "iteration: 6000.00 | cost: 0.00 | w: 199.87, b: 100.21 | dj_dw: -0.01, dj_db: 0.02\n",
      "iteration: 7000.00 | cost: 0.00 | w: 199.94, b: 100.10 | dj_dw: -0.00, dj_db: 0.01\n",
      "iteration: 8000.00 | cost: 0.00 | w: 199.97, b: 100.05 | dj_dw: -0.00, dj_db: 0.00\n",
      "iteration: 9000.00 | cost: 0.00 | w: 199.99, b: 100.02 | dj_dw: -0.00, dj_db: 0.00\n",
      "(w,b) found by gradient descent: (199.9929,100.0116)\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "\n",
    "x_train = np.array([1.0, 2.0])\n",
    "y_train = np.array([300, 500])\n",
    "# run gradient descent\n",
    "w_final, b_final = gradient_descent(x_train ,y_train, w_init, b_init, compute_cost, compute_gradient, tmp_alpha, iterations)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b3d06",
   "metadata": {},
   "source": [
    "### Make prediction using the values found by the parameters found by the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c5ccd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of price for 1000 sq.ft. is: ₹300004.42\n",
      "Prediction of price for 2000 sq.ft. is: ₹499997.27\n"
     ]
    }
   ],
   "source": [
    "f_wb_x1 = w_final * 1 + b_final\n",
    "print(f\"Prediction of price for 1000 sq.ft. is: ₹{1000 * f_wb_x1:.2f}\")\n",
    "\n",
    "f_wb_x2 = w_final * 2 + b_final\n",
    "print(f\"Prediction of price for 2000 sq.ft. is: ₹{1000 * f_wb_x2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5464aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
